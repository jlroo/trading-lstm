{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Strategy for Finance using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Configurations and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server. First, we import several widely used modules such as NumPy for numerical calculations, pandas for data management, matplotlib for visualizations, and TensorFlow for building and training deep neural networks.\n",
    "\n",
    "**Environment Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -l 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import h5py\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pprint as pp \n",
    "import tensorflow as tf \n",
    "from tensorflow.contrib import rnn\n",
    "import math\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tradingcore import prepareData as prepData\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting up random seed\n",
    "seed(42)\n",
    "tf.set_random_seed(42)\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.13.1\n",
      "Sklearn version: 0.20.1\n"
     ]
    }
   ],
   "source": [
    "# print versions\n",
    "print(\"Tensorflow version:\",tf.__version__)\n",
    "print(\"Sklearn version:\",sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical DL workflow starts with data preparation because the data is not clean and ready to use most of the time. Deep neural network building and training follow the data preparation. Lastly, the trained network is validated with a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data needs to be cleaned before training the network. Since cleaning the data takes significant amount of time (around 20 minutes), we have stored the cleaned data into another .h5 file. If you would like to use the original data and run the cleaning code, please set the \"usePreparedData\" variable to \"False\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data is prepared and stored in a seperate .h5 file.\n",
    "# Set usePreparedData = False to use the original data and run the data preparation code\n",
    "usePreparedData = True\n",
    "# insampleCutoffTimestamp variable is used to split the data in time into two pieces to create training and test set.\n",
    "insampleCutoffTimestamp = 1650\n",
    "\n",
    "# If usePreparatedData is True, then the prepared data is stored. Otherwise, the original data is stored\n",
    "if usePreparedData == True:\n",
    "    #with pd.HDFStore(\"/home/mimas/2sigma/DLI_FSI/2sigma/train_prepared.h5\", 'r') as train:\n",
    "    with pd.HDFStore(\"../data/algo_training/trainDataPrepared.h5\", 'r') as train:\n",
    "        df = train.get(\"train\") \n",
    "else:\n",
    "    with pd.HDFStore(\"../data/algo_training/train.h5\", 'r') as train:\n",
    "        df = train.get(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple instruments in the dataset and each instrument has an id. Time is represented by the 'timestamp' feature. Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>derived_0</th>\n",
       "      <th>derived_1</th>\n",
       "      <th>derived_2</th>\n",
       "      <th>derived_3</th>\n",
       "      <th>derived_4</th>\n",
       "      <th>fundamental_0</th>\n",
       "      <th>fundamental_1</th>\n",
       "      <th>fundamental_2</th>\n",
       "      <th>...</th>\n",
       "      <th>technical_43</th>\n",
       "      <th>technical_44</th>\n",
       "      <th>y</th>\n",
       "      <th>y_lagged</th>\n",
       "      <th>technical_diff</th>\n",
       "      <th>krnldiff</th>\n",
       "      <th>delta5diff</th>\n",
       "      <th>krnl40</th>\n",
       "      <th>delta540</th>\n",
       "      <th>fmod29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370326</td>\n",
       "      <td>-0.006316</td>\n",
       "      <td>0.222831</td>\n",
       "      <td>-0.213030</td>\n",
       "      <td>0.729277</td>\n",
       "      <td>-0.335633</td>\n",
       "      <td>0.113292</td>\n",
       "      <td>1.621238</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.011753</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014765</td>\n",
       "      <td>-0.038064</td>\n",
       "      <td>-0.017425</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>-0.034134</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.114285</td>\n",
       "      <td>-0.210185</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.001240</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>-0.050577</td>\n",
       "      <td>1.571245</td>\n",
       "      <td>-0.157525</td>\n",
       "      <td>-0.068550</td>\n",
       "      <td>-0.155937</td>\n",
       "      <td>1.060683</td>\n",
       "      <td>-0.764516</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.020940</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.006942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>-0.012705</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.037375</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.178495</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.007262</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.015959</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.176693</td>\n",
       "      <td>-0.025284</td>\n",
       "      <td>-0.057680</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.180894</td>\n",
       "      <td>0.139445</td>\n",
       "      <td>-0.125687</td>\n",
       "      <td>-0.018707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.007338</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.346856</td>\n",
       "      <td>0.166239</td>\n",
       "      <td>-1.482727</td>\n",
       "      <td>-0.992249</td>\n",
       "      <td>-0.125916</td>\n",
       "      <td>0.345812</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.584239</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.031425</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072036</td>\n",
       "      <td>0.014931</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.193205</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.032895</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300062</td>\n",
       "      <td>0.071251</td>\n",
       "      <td>-0.074451</td>\n",
       "      <td>-0.065292</td>\n",
       "      <td>-0.011286</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>0.210249</td>\n",
       "      <td>0.167494</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.015803</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003511</td>\n",
       "      <td>-0.034270</td>\n",
       "      <td>0.082372</td>\n",
       "      <td>-0.023937</td>\n",
       "      <td>-0.025750</td>\n",
       "      <td>0.007815</td>\n",
       "      <td>0.263451</td>\n",
       "      <td>-0.241212</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.027593</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.083330</td>\n",
       "      <td>0.081935</td>\n",
       "      <td>-1.482727</td>\n",
       "      <td>-0.206856</td>\n",
       "      <td>-0.839563</td>\n",
       "      <td>-0.234100</td>\n",
       "      <td>-0.291853</td>\n",
       "      <td>-2.522340</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435826</td>\n",
       "      <td>1.222721</td>\n",
       "      <td>0.363570</td>\n",
       "      <td>-0.005651</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.125375</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.292311</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.001899</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>-0.012705</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.037375</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.285388</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.193590</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.050219</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>-0.019258</td>\n",
       "      <td>0.055769</td>\n",
       "      <td>-0.084496</td>\n",
       "      <td>0.259828</td>\n",
       "      <td>0.198800</td>\n",
       "      <td>0.265104</td>\n",
       "      <td>-0.160462</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.018991</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>0.212615</td>\n",
       "      <td>-0.979520</td>\n",
       "      <td>-0.037375</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.395701</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.780873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.005203</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.024503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071704</td>\n",
       "      <td>-0.044019</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.038046</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.331778</td>\n",
       "      <td>-0.021114</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.006369</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116360</td>\n",
       "      <td>0.164506</td>\n",
       "      <td>0.156510</td>\n",
       "      <td>-0.129252</td>\n",
       "      <td>1.310059</td>\n",
       "      <td>-0.264846</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.370725</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.017768</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026824</td>\n",
       "      <td>-0.024105</td>\n",
       "      <td>-0.028991</td>\n",
       "      <td>0.433277</td>\n",
       "      <td>-0.006366</td>\n",
       "      <td>0.295138</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.285946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.367122</td>\n",
       "      <td>0.675543</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>-0.367778</td>\n",
       "      <td>-0.015529</td>\n",
       "      <td>0.129848</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.251624</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.008794</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453271</td>\n",
       "      <td>-0.036301</td>\n",
       "      <td>0.094657</td>\n",
       "      <td>-0.521106</td>\n",
       "      <td>0.049715</td>\n",
       "      <td>-0.352013</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.182694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.040724</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.122734</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>0.148015</td>\n",
       "      <td>2.493512</td>\n",
       "      <td>0.009746</td>\n",
       "      <td>-0.063111</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.162878</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.003921</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.062361</td>\n",
       "      <td>-0.063724</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>0.032845</td>\n",
       "      <td>0.174025</td>\n",
       "      <td>0.122335</td>\n",
       "      <td>-0.133188</td>\n",
       "      <td>-0.108286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.011317</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0.175489</td>\n",
       "      <td>-0.024445</td>\n",
       "      <td>-0.034079</td>\n",
       "      <td>-0.020533</td>\n",
       "      <td>0.282478</td>\n",
       "      <td>-0.069417</td>\n",
       "      <td>0.247840</td>\n",
       "      <td>0.303953</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.044167</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.008450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.190705</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>-0.056835</td>\n",
       "      <td>0.063464</td>\n",
       "      <td>-0.149507</td>\n",
       "      <td>0.267959</td>\n",
       "      <td>-0.392849</td>\n",
       "      <td>0.080084</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.048732</td>\n",
       "      <td>0.123793</td>\n",
       "      <td>0.266366</td>\n",
       "      <td>-0.009772</td>\n",
       "      <td>-0.248937</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.157415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.012101</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.001014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043156</td>\n",
       "      <td>0.540945</td>\n",
       "      <td>0.278394</td>\n",
       "      <td>0.563343</td>\n",
       "      <td>-0.144004</td>\n",
       "      <td>-0.024376</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.337090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.070837</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.003050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086832</td>\n",
       "      <td>-0.022962</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.071911</td>\n",
       "      <td>0.112818</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.001766</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.082307</td>\n",
       "      <td>-0.039761</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>-0.798021</td>\n",
       "      <td>0.149622</td>\n",
       "      <td>0.028448</td>\n",
       "      <td>0.062647</td>\n",
       "      <td>0.044142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.012542</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005351</td>\n",
       "      <td>-0.031086</td>\n",
       "      <td>0.078334</td>\n",
       "      <td>0.417328</td>\n",
       "      <td>-0.020146</td>\n",
       "      <td>0.390538</td>\n",
       "      <td>0.256594</td>\n",
       "      <td>-0.167223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.029697</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.054391</td>\n",
       "      <td>0.330190</td>\n",
       "      <td>-0.816961</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>-0.036462</td>\n",
       "      <td>-0.115197</td>\n",
       "      <td>-0.237428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.026302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003925</td>\n",
       "      <td>-0.037522</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.028244</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.049041</td>\n",
       "      <td>0.084827</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.016336</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.005712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.666596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710726</th>\n",
       "      <td>2100</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.079269</td>\n",
       "      <td>-0.021573</td>\n",
       "      <td>-0.033426</td>\n",
       "      <td>-0.333376</td>\n",
       "      <td>0.061463</td>\n",
       "      <td>0.217718</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.127397</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-0.050890</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>-0.009384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.453188</td>\n",
       "      <td>-0.009168</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710727</th>\n",
       "      <td>2101</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.204339</td>\n",
       "      <td>0.134338</td>\n",
       "      <td>0.143729</td>\n",
       "      <td>0.702932</td>\n",
       "      <td>-0.290513</td>\n",
       "      <td>0.031102</td>\n",
       "      <td>-0.243759</td>\n",
       "      <td>-0.205067</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.989691e+00</td>\n",
       "      <td>-0.001040</td>\n",
       "      <td>-0.003035</td>\n",
       "      <td>0.011515</td>\n",
       "      <td>0.004754</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>-0.002417</td>\n",
       "      <td>0.046869</td>\n",
       "      <td>-0.001460</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710728</th>\n",
       "      <td>2102</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.095661</td>\n",
       "      <td>-0.051734</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.004488</td>\n",
       "      <td>-0.249359</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266820e-11</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>-0.001172</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349820</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710729</th>\n",
       "      <td>2104</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.134436</td>\n",
       "      <td>-0.030198</td>\n",
       "      <td>-0.057608</td>\n",
       "      <td>0.016190</td>\n",
       "      <td>0.146911</td>\n",
       "      <td>-0.340176</td>\n",
       "      <td>-0.104457</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266820e-11</td>\n",
       "      <td>0.020394</td>\n",
       "      <td>0.003194</td>\n",
       "      <td>-0.002342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.185159</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710730</th>\n",
       "      <td>2107</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.180348</td>\n",
       "      <td>0.061339</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.049680</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.418095</td>\n",
       "      <td>-0.620948</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660443e-06</td>\n",
       "      <td>0.052314</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.578964</td>\n",
       "      <td>-0.020938</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710731</th>\n",
       "      <td>2108</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.247698</td>\n",
       "      <td>0.026739</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.280510</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.198069</td>\n",
       "      <td>-0.620398</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266820e-11</td>\n",
       "      <td>-0.019431</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>-0.003761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.150522</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710732</th>\n",
       "      <td>2109</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.425230</td>\n",
       "      <td>0.266186</td>\n",
       "      <td>0.075126</td>\n",
       "      <td>0.039726</td>\n",
       "      <td>-0.060703</td>\n",
       "      <td>-0.059133</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.286111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660443e-06</td>\n",
       "      <td>-0.065701</td>\n",
       "      <td>-0.012707</td>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.040423</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.262781</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710733</th>\n",
       "      <td>2114</td>\n",
       "      <td>1812</td>\n",
       "      <td>-2.225214</td>\n",
       "      <td>1.222721</td>\n",
       "      <td>1.571245</td>\n",
       "      <td>-0.347759</td>\n",
       "      <td>0.257406</td>\n",
       "      <td>0.172330</td>\n",
       "      <td>-0.098562</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266820e-11</td>\n",
       "      <td>-0.016923</td>\n",
       "      <td>-0.002151</td>\n",
       "      <td>-0.012710</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>-0.004449</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>0.243193</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710734</th>\n",
       "      <td>2117</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.136062</td>\n",
       "      <td>-0.006129</td>\n",
       "      <td>-1.482727</td>\n",
       "      <td>-0.316265</td>\n",
       "      <td>-0.480613</td>\n",
       "      <td>-0.270387</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.297367</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266820e-11</td>\n",
       "      <td>-0.057015</td>\n",
       "      <td>0.034297</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>-0.002306</td>\n",
       "      <td>0.015304</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>0.218020</td>\n",
       "      <td>-0.014916</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710735</th>\n",
       "      <td>2118</td>\n",
       "      <td>1812</td>\n",
       "      <td>-2.225214</td>\n",
       "      <td>1.222721</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.037375</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.204721</td>\n",
       "      <td>-0.136476</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.989691e+00</td>\n",
       "      <td>0.022479</td>\n",
       "      <td>-0.007932</td>\n",
       "      <td>-0.005968</td>\n",
       "      <td>-0.001756</td>\n",
       "      <td>0.004116</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>0.401249</td>\n",
       "      <td>-0.006091</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710736</th>\n",
       "      <td>2120</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>-0.012705</td>\n",
       "      <td>-0.398379</td>\n",
       "      <td>-0.037375</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.222814</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.554627</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>-0.032474</td>\n",
       "      <td>-0.033865</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>-0.003194</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>-0.003194</td>\n",
       "      <td>-0.272112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710737</th>\n",
       "      <td>2121</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.126793</td>\n",
       "      <td>0.040152</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.054294</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.423501</td>\n",
       "      <td>-0.416516</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266820e-11</td>\n",
       "      <td>0.043590</td>\n",
       "      <td>-0.001192</td>\n",
       "      <td>-0.002113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.023934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336718</td>\n",
       "      <td>-0.007716</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710738</th>\n",
       "      <td>2126</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.181550</td>\n",
       "      <td>-0.039677</td>\n",
       "      <td>-0.082102</td>\n",
       "      <td>-0.140690</td>\n",
       "      <td>-0.231005</td>\n",
       "      <td>-0.145413</td>\n",
       "      <td>-0.150222</td>\n",
       "      <td>0.158479</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.782362e+00</td>\n",
       "      <td>-0.010252</td>\n",
       "      <td>-0.003483</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>-0.002802</td>\n",
       "      <td>-0.002365</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.086226</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710739</th>\n",
       "      <td>2129</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.064731</td>\n",
       "      <td>-0.044927</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.615191</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.040299</td>\n",
       "      <td>-0.058827</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.051758e-05</td>\n",
       "      <td>0.034427</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>0.028929</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>-0.228275</td>\n",
       "      <td>-0.000905</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710740</th>\n",
       "      <td>2130</td>\n",
       "      <td>1812</td>\n",
       "      <td>-2.225214</td>\n",
       "      <td>1.222721</td>\n",
       "      <td>1.174958</td>\n",
       "      <td>-0.439014</td>\n",
       "      <td>-1.318172</td>\n",
       "      <td>-0.219093</td>\n",
       "      <td>0.087497</td>\n",
       "      <td>-1.461111</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.892265e-13</td>\n",
       "      <td>0.020224</td>\n",
       "      <td>-0.027963</td>\n",
       "      <td>0.010122</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>0.008168</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>0.414461</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710741</th>\n",
       "      <td>2131</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.149930</td>\n",
       "      <td>0.070203</td>\n",
       "      <td>0.363126</td>\n",
       "      <td>-0.010869</td>\n",
       "      <td>0.164914</td>\n",
       "      <td>-0.262682</td>\n",
       "      <td>-0.315310</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.279235e-09</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>-0.016629</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.018755</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.305399</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710742</th>\n",
       "      <td>2137</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.059745</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.062670</td>\n",
       "      <td>-1.842369</td>\n",
       "      <td>0.073483</td>\n",
       "      <td>0.287247</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>-0.017717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.043130</td>\n",
       "      <td>-0.002332</td>\n",
       "      <td>-0.035613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710743</th>\n",
       "      <td>2138</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.219608</td>\n",
       "      <td>-0.034336</td>\n",
       "      <td>-0.031817</td>\n",
       "      <td>0.270888</td>\n",
       "      <td>0.045701</td>\n",
       "      <td>0.134292</td>\n",
       "      <td>0.364995</td>\n",
       "      <td>0.064963</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660443e-06</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>-0.004487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.366031</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710744</th>\n",
       "      <td>2139</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.062132</td>\n",
       "      <td>0.070332</td>\n",
       "      <td>0.351273</td>\n",
       "      <td>0.478199</td>\n",
       "      <td>0.092099</td>\n",
       "      <td>0.494140</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.330590</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.892265e-13</td>\n",
       "      <td>0.027970</td>\n",
       "      <td>0.025162</td>\n",
       "      <td>-0.007581</td>\n",
       "      <td>-0.002689</td>\n",
       "      <td>-0.015003</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.075615</td>\n",
       "      <td>-0.006907</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710745</th>\n",
       "      <td>2140</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.213264</td>\n",
       "      <td>0.029603</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.273137</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.464013</td>\n",
       "      <td>-0.404886</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.279235e-09</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>-0.024362</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>1.009813</td>\n",
       "      <td>-0.006423</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710746</th>\n",
       "      <td>2142</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.223395</td>\n",
       "      <td>-0.042492</td>\n",
       "      <td>-0.060381</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>0.218667</td>\n",
       "      <td>-0.160979</td>\n",
       "      <td>-0.138038</td>\n",
       "      <td>0.348972</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660443e-06</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>-0.004077</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>-0.000775</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>-0.193525</td>\n",
       "      <td>-0.001304</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710747</th>\n",
       "      <td>2145</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.154051</td>\n",
       "      <td>-0.029331</td>\n",
       "      <td>-0.010545</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>-0.260369</td>\n",
       "      <td>-0.227888</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660443e-06</td>\n",
       "      <td>-0.032458</td>\n",
       "      <td>-0.000711</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.052745</td>\n",
       "      <td>-0.005793</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710748</th>\n",
       "      <td>2146</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.238458</td>\n",
       "      <td>0.316407</td>\n",
       "      <td>0.632261</td>\n",
       "      <td>0.531651</td>\n",
       "      <td>-0.154740</td>\n",
       "      <td>0.069316</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>-0.543269</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-0.004821</td>\n",
       "      <td>-0.017794</td>\n",
       "      <td>-0.024961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.029522</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.585441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710749</th>\n",
       "      <td>2148</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.089476</td>\n",
       "      <td>-0.038628</td>\n",
       "      <td>0.776538</td>\n",
       "      <td>3.759122</td>\n",
       "      <td>-0.177850</td>\n",
       "      <td>0.388419</td>\n",
       "      <td>0.193054</td>\n",
       "      <td>0.175415</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.999969e+00</td>\n",
       "      <td>-0.022806</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>-0.002914</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>-0.002499</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>-0.490991</td>\n",
       "      <td>-0.012937</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710750</th>\n",
       "      <td>2149</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.254593</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>-0.034742</td>\n",
       "      <td>-1.336611</td>\n",
       "      <td>-0.037401</td>\n",
       "      <td>0.244451</td>\n",
       "      <td>0.049125</td>\n",
       "      <td>-0.130919</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.892265e-13</td>\n",
       "      <td>-0.027780</td>\n",
       "      <td>-0.001462</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.003881</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>-0.185266</td>\n",
       "      <td>-0.008880</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710751</th>\n",
       "      <td>2150</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.123364</td>\n",
       "      <td>-0.055977</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.255730</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.328306e-10</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>-0.002948</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.272229</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710752</th>\n",
       "      <td>2151</td>\n",
       "      <td>1812</td>\n",
       "      <td>-2.225214</td>\n",
       "      <td>0.080905</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>3.369380</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.293557</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660443e-06</td>\n",
       "      <td>0.044597</td>\n",
       "      <td>-0.009241</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.121156</td>\n",
       "      <td>-0.010494</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710753</th>\n",
       "      <td>2154</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.077930</td>\n",
       "      <td>-0.038748</td>\n",
       "      <td>-0.031859</td>\n",
       "      <td>0.646608</td>\n",
       "      <td>-0.145526</td>\n",
       "      <td>-0.119539</td>\n",
       "      <td>-0.151587</td>\n",
       "      <td>-0.130524</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266820e-11</td>\n",
       "      <td>0.030816</td>\n",
       "      <td>-0.006852</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005879</td>\n",
       "      <td>0.043207</td>\n",
       "      <td>-0.001385</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710754</th>\n",
       "      <td>2156</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.269845</td>\n",
       "      <td>-0.005322</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.117539</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.214088</td>\n",
       "      <td>-0.307293</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.999260e+00</td>\n",
       "      <td>-0.011706</td>\n",
       "      <td>-0.000785</td>\n",
       "      <td>-0.029726</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>-0.032706</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.392242</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710755</th>\n",
       "      <td>2158</td>\n",
       "      <td>1812</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>-0.012705</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>-0.037375</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.065267</td>\n",
       "      <td>-1.059481</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>-0.011483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.026854</td>\n",
       "      <td>-0.003424</td>\n",
       "      <td>-0.522720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1710756 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  timestamp  derived_0  derived_1  derived_2  derived_3  \\\n",
       "0          10          0   0.370326  -0.006316   0.222831  -0.213030   \n",
       "1          11          0   0.014765  -0.038064  -0.017425   0.320652   \n",
       "2          12          0  -0.010622  -0.050577   1.571245  -0.157525   \n",
       "3          25          0  -0.003429  -0.012705  -0.005859  -0.037375   \n",
       "4          26          0   0.176693  -0.025284  -0.057680   0.015100   \n",
       "5          27          0   0.346856   0.166239  -1.482727  -0.992249   \n",
       "6          31          0   0.072036   0.014931  -0.005859   0.014063   \n",
       "7          38          0   0.300062   0.071251  -0.074451  -0.065292   \n",
       "8          39          0  -0.003511  -0.034270   0.082372  -0.023937   \n",
       "9          40          0  -0.083330   0.081935  -1.482727  -0.206856   \n",
       "10         41          0   0.435826   1.222721   0.363570  -0.005651   \n",
       "11         43          0  -0.003429  -0.012705  -0.005859  -0.037375   \n",
       "12         44          0   0.034991  -0.019258   0.055769  -0.084496   \n",
       "13         49          0  -0.003429   0.212615  -0.979520  -0.037375   \n",
       "14         54          0   0.071704  -0.044019  -0.005859   0.038046   \n",
       "15         59          0   0.116360   0.164506   0.156510  -0.129252   \n",
       "16         60          0   0.026824  -0.024105  -0.028991   0.433277   \n",
       "17         62          0   0.367122   0.675543  -0.008483  -0.367778   \n",
       "18         63          0   0.453271  -0.036301   0.094657  -0.521106   \n",
       "19         68          0  -0.122734   0.038939   0.148015   2.493512   \n",
       "20         69          0  -0.062361  -0.063724   0.021274   0.032845   \n",
       "21         70          0   0.175489  -0.024445  -0.034079  -0.020533   \n",
       "22         76          0  -0.190705   0.006221  -0.056835   0.063464   \n",
       "23         79          0   0.008503   0.048732   0.123793   0.266366   \n",
       "24         80          0   0.043156   0.540945   0.278394   0.563343   \n",
       "25         82          0   0.086832  -0.022962  -0.005859   0.006783   \n",
       "26         83          0  -0.082307  -0.039761   0.102000  -0.798021   \n",
       "27         85          0   0.005351  -0.031086   0.078334   0.417328   \n",
       "28         87          0   0.045536   0.054391   0.330190  -0.816961   \n",
       "29         90          0  -0.003925  -0.037522  -0.005859  -0.028244   \n",
       "...       ...        ...        ...        ...        ...        ...   \n",
       "1710726  2100       1812   0.079269  -0.021573  -0.033426  -0.333376   \n",
       "1710727  2101       1812  -0.204339   0.134338   0.143729   0.702932   \n",
       "1710728  2102       1812   0.095661  -0.051734  -0.005859   0.035816   \n",
       "1710729  2104       1812   0.134436  -0.030198  -0.057608   0.016190   \n",
       "1710730  2107       1812  -0.180348   0.061339  -0.005859   0.049680   \n",
       "1710731  2108       1812  -0.247698   0.026739  -0.005859  -0.280510   \n",
       "1710732  2109       1812  -0.425230   0.266186   0.075126   0.039726   \n",
       "1710733  2114       1812  -2.225214   1.222721   1.571245  -0.347759   \n",
       "1710734  2117       1812  -0.136062  -0.006129  -1.482727  -0.316265   \n",
       "1710735  2118       1812  -2.225214   1.222721  -0.005859  -0.037375   \n",
       "1710736  2120       1812  -0.003429  -0.012705  -0.398379  -0.037375   \n",
       "1710737  2121       1812   0.126793   0.040152  -0.005859  -0.054294   \n",
       "1710738  2126       1812  -0.181550  -0.039677  -0.082102  -0.140690   \n",
       "1710739  2129       1812  -0.064731  -0.044927  -0.005859   0.615191   \n",
       "1710740  2130       1812  -2.225214   1.222721   1.174958  -0.439014   \n",
       "1710741  2131       1812  -0.149930   0.070203   0.363126  -0.010869   \n",
       "1710742  2137       1812  -0.059745  -0.052186  -0.062670  -1.842369   \n",
       "1710743  2138       1812   0.219608  -0.034336  -0.031817   0.270888   \n",
       "1710744  2139       1812   0.062132   0.070332   0.351273   0.478199   \n",
       "1710745  2140       1812   0.213264   0.029603  -0.005859  -0.273137   \n",
       "1710746  2142       1812  -0.223395  -0.042492  -0.060381   0.016019   \n",
       "1710747  2145       1812  -0.154051  -0.029331  -0.010545   0.019339   \n",
       "1710748  2146       1812  -0.238458   0.316407   0.632261   0.531651   \n",
       "1710749  2148       1812   0.089476  -0.038628   0.776538   3.759122   \n",
       "1710750  2149       1812   0.254593   0.064668  -0.034742  -1.336611   \n",
       "1710751  2150       1812  -0.123364  -0.055977  -0.005859   0.010906   \n",
       "1710752  2151       1812  -2.225214   0.080905  -0.005859   3.369380   \n",
       "1710753  2154       1812  -0.077930  -0.038748  -0.031859   0.646608   \n",
       "1710754  2156       1812  -0.269845  -0.005322  -0.005859  -0.117539   \n",
       "1710755  2158       1812  -0.003429  -0.012705  -0.005859  -0.037375   \n",
       "\n",
       "         derived_4  fundamental_0  fundamental_1  fundamental_2    ...     \\\n",
       "0         0.729277      -0.335633       0.113292       1.621238    ...      \n",
       "1        -0.034134       0.004413       0.114285      -0.210185    ...      \n",
       "2        -0.068550      -0.155937       1.060683      -0.764516    ...      \n",
       "3         0.024913       0.178495       0.044287      -0.007262    ...      \n",
       "4         0.180894       0.139445      -0.125687      -0.018707    ...      \n",
       "5        -0.125916       0.345812       0.044287      -0.584239    ...      \n",
       "6         0.024913      -0.193205       0.044287       0.019531    ...      \n",
       "7        -0.011286       0.026365       0.210249       0.167494    ...      \n",
       "8        -0.025750       0.007815       0.263451      -0.241212    ...      \n",
       "9        -0.839563      -0.234100      -0.291853      -2.522340    ...      \n",
       "10        0.442866       0.125375       0.044287       0.292311    ...      \n",
       "11        0.024913      -0.285388       0.044287      -0.193590    ...      \n",
       "12        0.259828       0.198800       0.265104      -0.160462    ...      \n",
       "13        0.024913       0.395701       0.044287      -0.780873    ...      \n",
       "14        0.024913       0.331778      -0.021114       0.019531    ...      \n",
       "15        1.310059      -0.264846       0.044287       0.370725    ...      \n",
       "16       -0.006366       0.295138       0.044287      -0.285946    ...      \n",
       "17       -0.015529       0.129848       0.044287      -0.251624    ...      \n",
       "18        0.049715      -0.352013       0.044287      -0.182694    ...      \n",
       "19        0.009746      -0.063111       0.044287      -0.162878    ...      \n",
       "20        0.174025       0.122335      -0.133188      -0.108286    ...      \n",
       "21        0.282478      -0.069417       0.247840       0.303953    ...      \n",
       "22       -0.149507       0.267959      -0.392849       0.080084    ...      \n",
       "23       -0.009772      -0.248937       0.044287      -0.157415    ...      \n",
       "24       -0.144004      -0.024376       0.044287      -0.337090    ...      \n",
       "25        0.024913       0.071911       0.112818       0.019531    ...      \n",
       "26        0.149622       0.028448       0.062647       0.044142    ...      \n",
       "27       -0.020146       0.390538       0.256594      -0.167223    ...      \n",
       "28        0.012602      -0.036462      -0.115197      -0.237428    ...      \n",
       "29        0.024913      -0.049041       0.084827       0.019531    ...      \n",
       "...            ...            ...            ...            ...    ...      \n",
       "1710726   0.061463       0.217718       0.044287       0.127397    ...      \n",
       "1710727  -0.290513       0.031102      -0.243759      -0.205067    ...      \n",
       "1710728   0.024913      -0.004488      -0.249359       0.019531    ...      \n",
       "1710729   0.146911      -0.340176      -0.104457       0.002421    ...      \n",
       "1710730   0.024913      -0.418095      -0.620948       0.019531    ...      \n",
       "1710731   0.024913       0.198069      -0.620398       0.019531    ...      \n",
       "1710732  -0.060703      -0.059133       0.044287       0.286111    ...      \n",
       "1710733   0.257406       0.172330      -0.098562      -0.000398    ...      \n",
       "1710734  -0.480613      -0.270387       0.044287      -0.297367    ...      \n",
       "1710735   0.024913       0.204721      -0.136476       0.019531    ...      \n",
       "1710736   0.024913      -0.222814       0.044287      -0.554627    ...      \n",
       "1710737   0.024913      -0.423501      -0.416516       0.019531    ...      \n",
       "1710738  -0.231005      -0.145413      -0.150222       0.158479    ...      \n",
       "1710739   0.024913       0.040299      -0.058827       0.019531    ...      \n",
       "1710740  -1.318172      -0.219093       0.087497      -1.461111    ...      \n",
       "1710741   0.164914      -0.262682      -0.315310       0.019663    ...      \n",
       "1710742   0.073483       0.287247       0.044287       0.001499    ...      \n",
       "1710743   0.045701       0.134292       0.364995       0.064963    ...      \n",
       "1710744   0.092099       0.494140       0.044287      -0.330590    ...      \n",
       "1710745   0.024913      -0.464013      -0.404886       0.019531    ...      \n",
       "1710746   0.218667      -0.160979      -0.138038       0.348972    ...      \n",
       "1710747  -0.260369      -0.227888       0.006721       0.008255    ...      \n",
       "1710748  -0.154740       0.069316       0.044287      -0.543269    ...      \n",
       "1710749  -0.177850       0.388419       0.193054       0.175415    ...      \n",
       "1710750  -0.037401       0.244451       0.049125      -0.130919    ...      \n",
       "1710751   0.024913      -0.255730      -0.108285       0.019531    ...      \n",
       "1710752   0.024913      -0.293557       0.044287       0.019531    ...      \n",
       "1710753  -0.145526      -0.119539      -0.151587      -0.130524    ...      \n",
       "1710754   0.024913       0.214088      -0.307293       0.019531    ...      \n",
       "1710755   0.024913      -0.065267      -1.059481       0.019531    ...      \n",
       "\n",
       "         technical_43  technical_44         y  y_lagged  technical_diff  \\\n",
       "0       -2.000000e+00      0.000951 -0.011753  0.000046        0.000000   \n",
       "1       -2.000000e+00      0.000951 -0.001240  0.000046        0.000000   \n",
       "2       -2.000000e+00      0.000951 -0.020940  0.000046        0.006942   \n",
       "3       -2.000000e+00      0.000951 -0.015959  0.000046        0.006766   \n",
       "4        0.000000e+00      0.000951 -0.007338  0.000046        0.006236   \n",
       "5       -2.000000e+00      0.000951  0.031425  0.000046        0.010000   \n",
       "6       -2.000000e+00      0.000951 -0.032895  0.000046        0.006601   \n",
       "7       -2.000000e+00      0.000951  0.015803  0.000046        0.007909   \n",
       "8       -2.000000e+00      0.000951 -0.027593  0.000046        0.000000   \n",
       "9       -2.000000e+00      0.000951  0.006662  0.000046        0.000000   \n",
       "10      -2.000000e+00      0.000951 -0.001899  0.000046        0.000000   \n",
       "11      -2.000000e+00      0.000951  0.050219  0.000046        0.007407   \n",
       "12      -2.000000e+00      0.000951 -0.018991  0.000046        0.000000   \n",
       "13       0.000000e+00      0.000951 -0.005203  0.000046       -0.024503   \n",
       "14      -2.000000e+00      0.000951 -0.006369  0.000046        0.000000   \n",
       "15      -2.000000e+00      0.000951  0.017768  0.000046       -0.012122   \n",
       "16       0.000000e+00      0.000951 -0.001089  0.000046        0.007072   \n",
       "17      -2.000000e+00      0.000951 -0.008794  0.000046       -0.001446   \n",
       "18       0.000000e+00      0.000951  0.040724  0.000046       -0.000027   \n",
       "19      -2.000000e+00      0.000951 -0.003921  0.000046       -0.000538   \n",
       "20       0.000000e+00      0.000951 -0.011317  0.000046        0.000000   \n",
       "21      -2.000000e+00      0.000951  0.044167  0.000046        0.008450   \n",
       "22      -2.000000e+00      0.000951  0.001395  0.000046        0.000000   \n",
       "23       0.000000e+00      0.000951 -0.012101  0.000046       -0.001014   \n",
       "24       0.000000e+00      0.000951 -0.070837  0.000046       -0.003050   \n",
       "25      -2.000000e+00      0.000951 -0.001766  0.000046        0.008571   \n",
       "26       0.000000e+00      0.000951 -0.012542  0.000046       -0.004632   \n",
       "27       0.000000e+00      0.000951  0.029697  0.000046        0.012955   \n",
       "28       0.000000e+00      0.000951  0.030815  0.000046       -0.026302   \n",
       "29      -2.000000e+00      0.000951 -0.016336  0.000046        0.005712   \n",
       "...               ...           ...       ...       ...             ...   \n",
       "1710726 -3.330669e-16     -0.050890  0.008358 -0.009384        0.000000   \n",
       "1710727 -1.989691e+00     -0.001040 -0.003035  0.011515        0.004754   \n",
       "1710728 -1.266820e-11      0.005510 -0.001172  0.013182        0.000000   \n",
       "1710729 -1.266820e-11      0.020394  0.003194 -0.002342        0.000000   \n",
       "1710730 -1.660443e-06      0.052314 -0.019208  0.010236        0.000000   \n",
       "1710731 -1.266820e-11     -0.019431  0.000383 -0.003761        0.000000   \n",
       "1710732 -1.660443e-06     -0.065701 -0.012707  0.006391        0.003234   \n",
       "1710733 -1.266820e-11     -0.016923 -0.002151 -0.012710       -0.000356   \n",
       "1710734 -1.266820e-11     -0.057015  0.034297  0.002157       -0.002306   \n",
       "1710735 -1.989691e+00      0.022479 -0.007932 -0.005968       -0.001756   \n",
       "1710736 -2.000000e+00     -0.032474 -0.033865  0.021858       -0.003194   \n",
       "1710737 -1.266820e-11      0.043590 -0.001192 -0.002113        0.000000   \n",
       "1710738 -1.782362e+00     -0.010252 -0.003483  0.001174       -0.002802   \n",
       "1710739 -3.051758e-05      0.034427  0.005619  0.023354        0.002314   \n",
       "1710740 -6.892265e-13      0.020224 -0.027963  0.010122       -0.003100   \n",
       "1710741 -4.279235e-09      0.002708 -0.016629  0.006379        0.005736   \n",
       "1710742 -2.000000e+00     -0.024923  0.008800 -0.017717        0.000000   \n",
       "1710743 -1.660443e-06      0.010777  0.002015 -0.004487        0.000000   \n",
       "1710744 -6.892265e-13      0.027970  0.025162 -0.007581       -0.002689   \n",
       "1710745 -4.279235e-09      0.042746 -0.024362  0.013498        0.000000   \n",
       "1710746 -1.660443e-06      0.022390  0.008430 -0.004077       -0.001042   \n",
       "1710747 -1.660443e-06     -0.032458 -0.000711  0.000967        0.000000   \n",
       "1710748 -3.330669e-16     -0.004821 -0.017794 -0.024961        0.000000   \n",
       "1710749 -1.999969e+00     -0.022806 -0.001058 -0.002914        0.003262   \n",
       "1710750 -6.892265e-13     -0.027780 -0.001462  0.003725       -0.000977   \n",
       "1710751 -2.328306e-10      0.001004  0.004604  0.001954        0.002757   \n",
       "1710752 -1.660443e-06      0.044597 -0.009241  0.007506        0.000000   \n",
       "1710753 -1.266820e-11      0.030816 -0.006852  0.002378        0.000000   \n",
       "1710754 -1.999260e+00     -0.011706 -0.000785 -0.029726        0.003938   \n",
       "1710755  0.000000e+00      0.000951  0.003497 -0.011483        0.000000   \n",
       "\n",
       "         krnldiff  delta5diff    krnl40  delta540    fmod29  \n",
       "0        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "1        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "2        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "3        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "4        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "5        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "6        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "7        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "8        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "9        0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "10       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "11       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "12       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "13       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "14       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "15       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "16       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "17       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "18       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "19       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "20       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "21       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "22       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "23       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "24       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "25       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "26       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "27       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "28       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "29       0.000000    0.000000 -0.041838  0.000011  0.666596  \n",
       "...           ...         ...       ...       ...       ...  \n",
       "1710726  0.000000    0.000000 -0.453188 -0.009168 -0.034480  \n",
       "1710727  0.006968   -0.002417  0.046869 -0.001460 -0.034480  \n",
       "1710728  0.005418    0.000000  0.349820 -0.000827 -0.034480  \n",
       "1710729  0.000000    0.000000 -0.185159  0.000049 -0.034480  \n",
       "1710730  0.004216    0.002716  0.578964 -0.020938 -0.034480  \n",
       "1710731  0.000000    0.000575  0.150522  0.005653 -0.034480  \n",
       "1710732  0.040423    0.003234  0.262781  0.003433 -0.034480  \n",
       "1710733 -0.004449   -0.000356  0.243193  0.006373 -0.034480  \n",
       "1710734  0.015304   -0.001026  0.218020 -0.014916 -0.034480  \n",
       "1710735  0.004116   -0.001219  0.401249 -0.006091 -0.034480  \n",
       "1710736  0.003268   -0.003194 -0.272112  0.000000 -0.034480  \n",
       "1710737 -0.023934    0.000000  0.336718 -0.007716 -0.034480  \n",
       "1710738 -0.002365   -0.000005 -0.086226 -0.001102 -0.034480  \n",
       "1710739  0.028929    0.003790 -0.228275 -0.000905 -0.034480  \n",
       "1710740  0.008168   -0.003100  0.414461  0.008308 -0.034480  \n",
       "1710741  0.018755    0.005736  0.305399  0.005956 -0.034480  \n",
       "1710742 -0.043130   -0.002332 -0.035613  0.000000 -0.034480  \n",
       "1710743  0.000000    0.000000 -0.366031 -0.001975 -0.034480  \n",
       "1710744 -0.015003    0.000150  0.075615 -0.006907 -0.034480  \n",
       "1710745  0.000000    0.002586  1.009813 -0.006423 -0.034480  \n",
       "1710746 -0.000775   -0.001042 -0.193525 -0.001304 -0.034480  \n",
       "1710747  0.000000    0.002064  0.052745 -0.005793 -0.034480  \n",
       "1710748 -0.029522    0.000741  0.585441  0.000000 -0.034480  \n",
       "1710749 -0.002499    0.000534 -0.490991 -0.012937 -0.034480  \n",
       "1710750 -0.003881    0.001808 -0.185266 -0.008880 -0.034480  \n",
       "1710751 -0.002948    0.002757  0.272229  0.005969 -0.034480  \n",
       "1710752  0.007508    0.000899  0.121156 -0.010494 -0.034480  \n",
       "1710753  0.000000   -0.005879  0.043207 -0.001385 -0.034480  \n",
       "1710754 -0.032706    0.003938  0.392242  0.003202 -0.034480  \n",
       "1710755 -0.026854   -0.003424 -0.522720  0.000000 -0.034480  \n",
       "\n",
       "[1710756 rows x 114 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will print the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original data is stored, the data preparation code will be executed in the following cell. First, extreme values in each feature set are removed. Then, some hand-crafted features are added to feature set to boost the prediction accuracy. There are many methods including PCA and auto-encoders to do the feature engineering rather than creating hand-crafted features. As an exercise, we highly recommend you to add auto-encoders to the code and check the accuracy after the lab. Lastly, NaNs are replaced with the median of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if usePreparedData == False:\n",
    "    # Original data is not clean and some the samples are a bit extreme.\n",
    "    # These values are removed from the feature set.\n",
    "    df = prepData.removeExtremeValues(df, insampleCutoffTimestamp)\n",
    "    # A little bit feature engineering. Hand-crafted features are created here to boost the accuracy.\n",
    "    df = prepData.createNewFeatures(df) \n",
    "    # Check whether ve still have any NaNs \n",
    "    df = prepData.fillNaNs(df) \n",
    "    df.to_hdf(\"data/algo_trading/trainDataPrepared.h5\", 'train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Strategy for Finance using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction\n",
    "\n",
    "Now, we set up the TensorFlow compute graph. The deep neural network that is used in this code is comprised of a LSTM cell that runs over 10 time steps, a fully connected layers (FCL), and also drop-out layers to prevent overfitting. Calculating the number of time steps for a recurrent neural network is not a trivial task. It is actually another hyperparameter that needs to be searched. The network is depicted in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/dnn.jpg\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Figure 3: Structure of the LSTM based deep neural network\n",
    "                                \n",
    "Below is the code to build the deep neural network depicted in Figure 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape): \n",
    "    initial = tf.truncated_normal(shape, stddev=0.3)\n",
    "    return tf.Variable(initial) \n",
    "    \n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.3)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "n_time_steps = 10\n",
    "def getDNN (x, LSTMCellSize, keep_prob):\n",
    "    with tf.name_scope('model'):\n",
    "        with tf.name_scope('RNN'):\n",
    "            # We will add two dropout layers and LSTM cells with the number of units as LSTMCellSize.\n",
    "            cell = rnn.DropoutWrapper(rnn.BasicLSTMCell(LSTMCellSize, forget_bias=2, activation=tf.nn.tanh), output_keep_prob=keep_prob)\n",
    "            # We use the cell to create RNN.\n",
    "            # Note that outputs is not a tensor, it is a list with one element which is numpy array. \n",
    "            outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32) \n",
    "            outputs_shape = outputs.get_shape().as_list()\n",
    "                \n",
    "        # hidden layer with sigmoid activation\n",
    "        with tf.name_scope('W_fc1'):\n",
    "            W_fc1 = weight_variable([LSTMCellSize, 1])\n",
    "        with tf.name_scope('b_fc1'):\n",
    "            b_fc1 = bias_variable([1])\n",
    "        with tf.name_scope('pred'):\n",
    "            pred = tf.matmul(outputs[:,-1,:], W_fc1) + b_fc1\n",
    "\n",
    "        return pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column names that will be included in the featureset are added into colList.\n",
    "# colList will be used throughout the lab.\n",
    "colList=[]                  \n",
    "for thisColumn in df.columns: \n",
    "    if thisColumn not in ('id', 'timestamp', 'y', 'CntNs', 'y_lagged'): \n",
    "        colList.append(thisColumn)\n",
    "colList.append('y_lagged')\n",
    "\n",
    "#if you do not reset the default graph you will need to restart the kernel\n",
    "#every time this notebook is run\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network Parameters \n",
    "# Number of units in the LSTM cell.\n",
    "n_LSTMCell = len(colList)\n",
    "\n",
    "# Placeholder for the input and the keep probability for the dropout layers\n",
    "with tf.name_scope('input'):\n",
    "    x= tf.placeholder(tf.float32, shape=[None, n_time_steps, len(colList)])\n",
    "with tf.name_scope('keep_prob'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# At the input, we create 2-layer LSTM cell (with dropout layers)\n",
    "print('Building tensorflow graph')\n",
    "\n",
    "# Graph construction for the LSTM based deep neural network. \n",
    "# Structure of the network is depicted in the above figure.\n",
    "# Please see the dnn.py to see the code of the network.\n",
    "pred = getDNN (x, n_LSTMCell, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into two pieces in time to have a training and testing set. In order to have enough sample for each id, the cut-off timestamp for the training set was defined in \"insampleCutoffTimestamp\" variable as 1650. Figure 4 shows how an instrument is split in time to create training and testing set. While training the model, the training set for each instrument will be fed separately to learn the time patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/data_split.jpg\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    Figure 4: Training and Testing Dataset\n",
    "    \n",
    "In the Kaggle challenge, the metric to evaluate the prediction accuracy was given as Pearson correlation. In statistics, [pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is a measure of the linear correlation between two variables X and Y. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation. It is widely used in the sciences. It was developed by Karl Pearson from a related idea introduced by Francis Galton in the 1880s.\n",
    "\n",
    "Depending on the frequency of the financial data, Pearson correlation (R) can be very small. In finance, given the high ratio of signal-to-noise, even a small R can deliver meaningful value. Please note that the algorithm that won the challenge had only 0.038 R.\n",
    "\n",
    "The following cell includes the code for creating training and testing set, and calculating Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for the output (label)\n",
    "with tf.name_scope('label'):\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1]) \n",
    "# Placeholder to be able to split the data into training and test set while training the network.\n",
    "inSampleCutoff = tf.placeholder(tf.int32, shape = ())\n",
    "\n",
    "# this is important - we only want to train on the in-sample set of rows using TensorFlow\n",
    "y_inSample = y[0:inSampleCutoff]\n",
    "pred_inSample = pred[0:inSampleCutoff]\n",
    "\n",
    "# also extract out of sample predictions and actual values,\n",
    "# we'll use them for evaluation while training the model.\n",
    "y_outOfSample = y[inSampleCutoff:]\n",
    "pred_outOfSample = pred[inSampleCutoff:]\n",
    "\n",
    "with tf.name_scope('stats'):\n",
    "    # Pearson correlation to evaluate the model\n",
    "    covariance = tf.reduce_sum(tf.matmul(tf.transpose(tf.subtract(pred_inSample, tf.reduce_mean(pred_inSample))),tf.subtract(y_inSample, tf.reduce_mean(y_inSample))))\n",
    "    var_pred = tf.reduce_sum(tf.square(tf.subtract(pred_inSample, tf.reduce_mean(pred_inSample))))\n",
    "    var_y = tf.reduce_sum(tf.square(tf.subtract(y_inSample, tf.reduce_mean(y_inSample))))\n",
    "    pearson_corr = covariance / tf.sqrt(var_pred * var_y) \n",
    "\n",
    "tf.summary.scalar(\"pearson_corr\", pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the traditional machine learning and deep learning methods, it is assumed that the feature set and predicted value have zero mean and unit variance gaussian distribution. Empirical studies show that the financial data such as asset returns is often not compatible with this assumption. That is why we normalize the \"y\" variable by subtracting its mean and dividing the result by the standard deviation in the following cell. As an exercise, you can also normalize the features and see if you improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset is also created here. We included the code to split the data in the above cell. \n",
    "# The difference is that the above code will be used in the training by the TensorFlow.\n",
    "# This code will not be used by TensorFlow and creates the testing dataset whenever it is executed.\n",
    "dfInSample = df[df.timestamp <  insampleCutoffTimestamp]\n",
    "# create a reference dataframe (that only depends on in-sample data)\n",
    "# that gives us standard deviation and mean information on per-id basis\n",
    "# we'll use it later for variance stabilization\n",
    "meanStdById = dfInSample.groupby(['id']).agg( {'y':['mean', 'std']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the graph for training the model and see intermediate diagnostics results and the final result. We defined the important hyperparameters including the epoch, training batch size and learning rate at the top of the cell. Initially, the epoch is set to 1 because it takes 15-20 minutes to complete the training with 10 epochs even though we are using GPUs. In order to speed up the training in the lab environment, we provided pre-trained networks with 10 epochs and 20 epochs. An adaptive learning rate starting from 0.002 with exponential decay is used for the training from scratch. Learning rate should be set to 0.00058 and 0.00061 for using pre-trained models with 10 and 15 epochs respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "display_step = 100 \n",
    "epoch = 1\n",
    "pre_trained_model = 'tasks/task2/task/SavedModels/model_epoch_10.ckpt'\n",
    "mini_batch_limit = 1300\n",
    "\n",
    "# set up adaptive learning rate:\n",
    "globalStep = tf.placeholder(tf.float32)\n",
    "# Ratio of globalStep / totalDecaySteps is designed to indicate how far we've progressed in training.\n",
    "# the ratio is 0 at the beginning of training and is 1 at the end.\n",
    "# adaptiveLearningRate will thus change from the starting learningRate to learningRate * decay_rate\n",
    "# in order to simplify the code, we are fixing the total number of decay steps at 1 and pass globalStep\n",
    "# as a fraction that starts with 0 and tends to 1.\n",
    "# Learning rate should be set to 0.002 if you are training from scratch.\n",
    "# Learning rate should be set to 0.00058 if you are using the pre-trained network with 10 epochs.\n",
    "# Learning rate should be set to 0.00061 if you are using the pre-trained network with 15 epochs.\n",
    "adaptiveLearningRate = tf.train.exponential_decay(\n",
    "  0.00058,       # Start with this learning rate\n",
    "  globalStep,  # globalStep / totalDecaySteps shows how far we've progressed in training\n",
    "  1,           # totalDecaySteps\n",
    "  0.3)         # decay_rate, the factor by which the starting learning rate will be \n",
    "               # multiplied when the training is finished\n",
    "    \n",
    "# Define loss and optimizer\n",
    "# Note the loss only involves in-sample rows\n",
    "# Regularization is added in the loss function to avoid over-fitting\n",
    "rnn_variables = lstm_variables = [v for v in tf.trainable_variables()\n",
    "                    if v.name.startswith('rnn')]\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.nn.l2_loss(tf.subtract(y_inSample,pred_inSample)) + tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(scale=0.0001), tf.trainable_variables())\n",
    "\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=adaptiveLearningRate).minimize (loss) \n",
    "\n",
    "# Getting unique ids to train the network per id basis.\n",
    "ids = df.id.unique()\n",
    "ids.sort()\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "# initialize the variables \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "totalActual = []\n",
    "totalPredicted = []\n",
    "import random\n",
    "# Launch the graph \n",
    "# Implement Cross Validation, but in a vay that preserves temporal structure for id's \n",
    "with tf.Session() as sess:  \n",
    "    # Global variables are initialized\n",
    "    sess.run(init) \n",
    "    \n",
    "    # Restore latest checkpoint\n",
    "    model_saver = tf.train.Saver()\n",
    "    model_saver.restore(sess, pre_trained_model)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"tasks/tensorboard/logs\", graph=tf.get_default_graph())\n",
    "    step = 50  \n",
    "    writer_step = 1;\n",
    "    for i in range(epoch):\n",
    "        print('Epoch: ', i, '******************************')        \n",
    "        actual = []\n",
    "        predicted = []\n",
    "        \n",
    "        random.shuffle(ids)\n",
    "\n",
    "        for thisId in ids:\n",
    "            # Getting the data of the current id\n",
    "            this_df = df[df.id == thisId].copy()\n",
    "            this_df = this_df.sort_values(['id', 'timestamp'])\n",
    "                        \n",
    "            # we need to pass training set to the graph definition\n",
    "            # optimization will only consider in training set\n",
    "            inSampleSize, _ = this_df[this_df.timestamp < insampleCutoffTimestamp].shape\n",
    "            totalRows, _ = this_df.shape\n",
    "            \n",
    "            batch_y = this_df.loc[:,'y'].values            \n",
    "            batch_x = this_df[colList].values\n",
    "                    \n",
    "            if totalRows < n_time_steps:\n",
    "                continue\n",
    "\n",
    "            # Data is formated as a 3D tensor with the shape of (batch_size, n_time, n_feature) for LSTM\n",
    "            # n_time_steps parameter determines how many steps that LSTM will unroll in time\n",
    "            complete_x = np.zeros([totalRows-n_time_steps+1, n_time_steps, len(colList)])\n",
    "            for n in range(n_time_steps):\n",
    "                complete_x[:,n,:]=batch_x[n:totalRows-n_time_steps+n+1,:]\n",
    "            \n",
    "            batch_y = batch_y[n_time_steps-1:]\n",
    "            inSampleSize -= n_time_steps - 1\n",
    "\n",
    "            # variance stabilizing transform\n",
    "            # some id's will not have in-sample rows, we cannot perform transform on those\n",
    "            # furthermore, since there is not in-sample rows to train on, we must skip\n",
    "            if inSampleSize < 10:\n",
    "                continue\n",
    "                \n",
    "            # perform variance stabilization\n",
    "            thisMean = meanStdById.loc[thisId][0]\n",
    "            thisStd = meanStdById.loc[thisId][1]\n",
    "            batch_y = (batch_y - thisMean) / thisStd\n",
    "            \n",
    "            batch_y = batch_y.reshape(-1,1)\n",
    "            minibatchSize, _ = batch_y.shape\n",
    "\n",
    "            # we want to make sure that RNN reaches steady state\n",
    "            if minibatchSize < mini_batch_limit: \n",
    "                continue \n",
    "            \n",
    "            # Run optimization \n",
    "            # note: keep_prob is set to 0.5 for training only!\n",
    "            _, currentRate = sess.run([optimizer, adaptiveLearningRate], feed_dict={x: complete_x, y: batch_y, keep_prob:0.5, inSampleCutoff:inSampleSize, globalStep:i/epoch})\n",
    "\n",
    "            # Obtain out of sample target variable and our prediction\n",
    "            y_oos, pred_oos = sess.run([y_outOfSample, pred_outOfSample], feed_dict={x: complete_x, y: batch_y, keep_prob:1.0, inSampleCutoff:inSampleSize}) \n",
    "            \n",
    "            # flatten the returned lists\n",
    "            y_oos = [y for x in y_oos for y in x]\n",
    "            pred_oos = [y for x in pred_oos for y in x]\n",
    "            \n",
    "            #reverse transform before recording the results\n",
    "            if inSampleSize:            \n",
    "                y_oos = [ (t*thisStd + thisMean) for t in y_oos]\n",
    "                pred_oos = [ (t*thisStd + thisMean) for t in pred_oos]\n",
    "            \n",
    "            # record the results\n",
    "            actual.extend(y_oos)\n",
    "            predicted.extend(pred_oos)\n",
    "                       \n",
    "            totalActual.extend(y_oos)\n",
    "            totalPredicted.extend(pred_oos)\n",
    "            \n",
    "            # Once every display_step show some diagnostics - the loss function, in-sample correlation, etc.\n",
    "            if step % display_step == 0: \n",
    "                # Calculate batch accuracy \n",
    "                # Calculate batch loss \n",
    "                correl, lossResult, summary = sess.run([pearson_corr, loss, summary_op], feed_dict={x: complete_x, y: batch_y, keep_prob:1.0, inSampleCutoff:inSampleSize})\n",
    "                \n",
    "                writer.add_summary(summary, writer_step)\n",
    "                writer_step += 1\n",
    "                # corrcoef sometimes fails to compute correlation for a perfectly valid reason (e.g. stdev(pred_oos) is 0)\n",
    "                # it sets the result to nan, but also gives an annoying warning\n",
    "                # the following suppresses the warning\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    correl_oos = np.corrcoef(y_oos, pred_oos)[0,1]\n",
    "                    \n",
    "                print('LR: %s - Iter %s, minibatch loss = %s, minibatch corr = %s, oos %s (%s/%s)' % (currentRate, step, lossResult, correl, correl_oos, inSampleSize, totalRows))\n",
    "                \n",
    "            step += 1 \n",
    "       \n",
    "        print('Optimization Finished!') \n",
    "        print('Correl: ', np.corrcoef(actual, predicted)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes 3-5 minutes to complete the training with 1 epochs. We also provided TensorBoard to review the model architecture, loss and correlation variables. TensorBoard is a suite of web applications for inspecting and understanding your TensorFlow runs and graphs. \n",
    "\n",
    "### Click [here](/tensorboard/) to start TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a correlation value around R = 0.04. Note that the correlation tends to increase with each epoch (but not always). \n",
    "\n",
    "Let's use the pre-trained model with 15 epochs by setting the pre_trained_model variable as **pre_trained_model = '/tasks/task2/task/SavedModels/model_epoch_15.ckpt'** in above cell, lower the starting Learning Rate to **0.00061** and re-run everything using Kernel->Restart & Run All.\n",
    "\n",
    "What is the correlation that you get this time? Was it improved?\n",
    "\n",
    "Since training takes significant amount of time, we recommend you train the model with 20 epochs and check the correlation after this lab in your environment. You should get a correlation value around R = 0.05.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Strategy for Finance using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise\n",
    "Please read ahead and only come back to these optional exercises if time permits.\n",
    "\n",
    "**Train from scratch** [20-30 mins]\n",
    "\n",
    "First, change the # of epochs to 20 in the above cell. Second, put the starting learning rate back to **0.002**. Third, comment out the two line where the pre-trained model is loaded (under \"Restore latest checkpoint\"). Then re-run everything using Kernel->Restart & Run All. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can a portfolio manager assess the predicted signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could scatter-plot actual returns over the predicted returns, however correlation is not visually apparent on scatter plots when the correlation is below 20-30%. The correlation we achieve in this signal is much weaker which is typical of modern financial markets. Correlations which we often observe in other applications of predictive models are all but impossible in the financial markets which are highly efficient (simply put, unpredictable). If we imagine that someone has a signal with correlation of 30% using leverage the person would soon get extremely rich - and the observed signal (inefficiency) would disappear from the market.\n",
    "\n",
    "In order to visually assess the signal, we split out of sample data points into buckets based on the value of predicted returns. We then compute per-bucket mean actual returns. Then we plot mean actual returns (Y axis) against predicted returns (X axis). We thus plot one point per bucket. By taking mean value, we average out the variance within each bucket and uncover the predictive value of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = totalActual\n",
    "predicted = totalPredicted\n",
    "\n",
    "actualMeanReturn = []\n",
    "predictedMeanReturn = []\n",
    "stdActualReturns = []\n",
    "# Buckets are created\n",
    "buckets = np.arange(-0.02,0.02,0.002)\n",
    "\n",
    "actual = np.array(actual)\n",
    "predicted = np.array(predicted)\n",
    "\n",
    "# Predicted values and the actual values are placed into buckets\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for i in range(len(buckets)-1):\n",
    "        index = np.logical_and(predicted>buckets[i], predicted<buckets[i+1])\n",
    "        thisBucket = actual[index].mean()\n",
    "        actualMeanReturn.append(thisBucket)\n",
    "        predictedMeanReturn.append(predicted[index].mean())\n",
    "        stdActualReturns.append(actual[index].std())\n",
    "\n",
    "# Actual versus predicted values are plotted\n",
    "plt.figure()\n",
    "plt.plot(predictedMeanReturn,actualMeanReturn, marker='*')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(predictedMeanReturn, actualMeanReturn, yerr = stdActualReturns, marker='*')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How much variance is there?**\n",
    "\n",
    "Plot 2 answers this question by adding error bars to the previous plot. Length of the error bar is equal to the standard deviation of actual returns within each respective bucket.\n",
    "Plots such as these would be typically used by a portfolio manager to assess behavior of prospective signals and to assess signal levels at which an action should be taken. The simplest trading system utilizing this signal would buy security when predicted return is above some threshold (say, above 0.5%) and sell (or short-sell) the security when the signal is below negative threshold (e.g. below -0.5%). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following steps after the lab.\n",
    "\n",
    "1. Try using other machine learning techniques such as random forest, ridge regression, xgboost and compare the correlation with LSTM based predictor.\n",
    "\n",
    "2. Try using autoencoder to extract fewer features than the original dataset provides and use the features as input to the deep learning model. Analyze the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, step by step implementation of a LSTM based deep neural network to predict time series financial data is presented. The performance of the model is evaluated with the pearson correlation and competitive performance is achieved. The code provided in this lab can be used in complex trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Post-Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, don't forget to save your work from this lab before time runs out and the instance shuts down!!\n",
    "\n",
    "1. You can download the data from this [https://www.kaggle.com/c/two-sigma-financial-modeling](https://www.kaggle.com/c/two-sigma-financial-modeling).\n",
    "\n",
    "2. To use the data, please set the \"usePreparedData\" variable to False before running the code on your environment.\n",
    "\n",
    "3. Also, remove the code \"model_saver.restore(sess, pre_trained_model)\" to train the model for you data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
